{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2024 Semester 1\n",
    "\n",
    "## Assignment 1: Wine quality classification with K-NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student ID(s):**     `1356056`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K-NN classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0]\n",
      "[1. 1. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
      "[0. 1. 0. 1. 1. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "wine_train = np.genfromtxt('COMP30027_2024_asst1_data/winequality-train.csv', delimiter=',', skip_header=1)\n",
    "wine_test = np.genfromtxt('COMP30027_2024_asst1_data/winequality-test.csv', delimiter=',', skip_header=1)\n",
    "\n",
    "# Extract features (X) and labels (Y)\n",
    "train_X = wine_train[:10, :-1]\n",
    "train_Y = wine_train[:10, -1]\n",
    "\n",
    "test_X = wine_test[:10, :-1]\n",
    "test_Y = wine_test[:10, -1]\n",
    "\n",
    "def euclideanDistance(train, test):\n",
    "    return np.sqrt(np.sum((train - test)**2))\n",
    "\n",
    "\n",
    "def knnClassifier(train_X, train_Y, test_X, k):\n",
    "    test_Y = []\n",
    "    for test_row in test_X:\n",
    "        #Computing euclidean distances for each point\n",
    "        distances = []\n",
    "        for train_row in train_X:\n",
    "            distances.append(euclideanDistance(train_row, test_row))\n",
    "\n",
    "        #Finding the k closest rows that are closest to the test row\n",
    "        index_of_closest = np.argsort(distances)\n",
    "        labels_of_closest = []\n",
    "        for i in range(k):\n",
    "            labels_of_closest.append(train_Y[index_of_closest[i]])\n",
    "\n",
    "        #Counting labels of k closest neighbours\n",
    "        max_count = 0\n",
    "\n",
    "        majority_label = None\n",
    "        for label in set(labels_of_closest):\n",
    "            count = sum(1 for x in labels_of_closest if x == label)\n",
    "            if count > max_count:\n",
    "                max_count = count\n",
    "                majority_label = label\n",
    "            elif count == max_count and count > 1:\n",
    "                majority_label = labels_of_closest[0]\n",
    "        test_Y.append(majority_label)\n",
    "    return test_Y\n",
    "\n",
    "\n",
    "knn5 = KNeighborsClassifier(n_neighbors=6)\n",
    "knn1 = KNeighborsClassifier(n_neighbors=1)\n",
    "    \n",
    "knn5.fit(train_X, train_Y)\n",
    "knn1.fit(train_X, train_Y)\n",
    "    \n",
    "print(knnClassifier(train_X, train_Y, test_X, k = 6))\n",
    "print(knn5.predict(test_X))\n",
    "print(knn1.predict(test_X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 1-NN classification\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalization\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model extensions\n",
    "\n",
    "#### NOTE: you may develope codes or functions to help respond to the question here, but your formal answer must be submitted separately as a PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1\n",
    "Compare the performance of your best 1-NN model from Question 3 to a Gaussian naive Bayes model on this dataset (you may use library functions to implement the Gaussian naive Bayes model). In your write-up, state the accuracy of the naive Bayes model and identify instances where the two models disagree. Why do the two models classify these instances differently?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2\n",
    "Implement two additional distance measures for your K-NN model: cosine similarity and Mahalanobis distance (you may use library functions for these distance measures). Do 1-NN classification using each of these new distance measures and the three normalization options from Question 3. Discuss how the new distance metrics compare to Euclidean distance and how each metric is affected by normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3\n",
    "Implement either of the two K-NN weighting strategies discussed in lecture (inverse linear distance or inverse distance). Compare the performance of the weighted and majority vote models for a few different values of K. In your write-up, discuss how weighting strategy and the value of K affect the model's decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4\n",
    "Measure the empirical distribution of class labels in the training dataset (what percentage of the training data comes from each class). Then evaluate the distribution of labels predicted by your K-NN model for the test data, for a range of values for K. Does the class distribution of the predicted labels match the class distribution of the training data? Explain why or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
